{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a82c73e-f0c5-4d0a-be41-d40f21449d6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "###  Board and Committee Meetings   \n",
       "   \n",
       "We start with the [videos recorded by the local public access cable channel](https://acmi.tv/programs/government/) for government meetings, create a transcript using [OpenAI Whisper](https://github.com/openai/whisper), ask a series of questions, one for each of the published meeting agenda items, and follow-up questions depending on the summary of each item, using LangChains text splitters and QA retrieval pipelines.  The raw transcript is further organized by agenda item and speaker in markdown format; to be automated.  The raw transcript and markdown are saved as text columns in the ```governance.meetings``` table, while the Q&A is stored as a dictionary.\n",
       "\n",
       "\n",
       "This [notebook](\"transcripts\\ ETL.ipynb\") shows the components and is intended to be used with the *polis* postgres database available.  Adapting for stand alone use should be straighforward.\n",
       "\n",
       "\n",
       "#### Raw Transcript\n",
       "\n",
       "The raw transcript using [OpenAI Whisper](https://github.com/openai/whisper) is about 25,000 words, 125,000 characters for a 3 hour meeting.  The OpenAI api calls to create the transcripts run from about 0.90 to $1.00 per meeting.\n",
       "\n",
       "Meetings held via Zoom have poor transcription when compared to in-person meetings; where remote participants also are sometimes garbled.\n",
       "\n",
       "The raw transcript is one long paragraph.  Our choice of text splitters in langchain's RecursiveCharacterTextSplitter module bears scrutiny.\n",
       "\n",
       "We store the raw transcripts for further internal processing or by agents.\n",
       "\n",
       "\n",
       "#### 20 Questions\n",
       "\n",
       "We use the OpenAI chat llm and Langchain's RetrievalQA to ask questions.  We can prepare the questions from asking for a summary of the entire meeting, a summary for each agenda item and follow-up questions where appropriate.  The chat does poorly on answering questions such as \"list everyone who spoke in the meeting\" due to context limitations, but does very well in asking detailed questions about a topic. We ask pre-determined questions such as results of any votes expected.  \n",
       "\n",
       "These questions can all be prepared in advance of the meeting.\n",
       "\n",
       "Answers from the OpenAI chat gpt-4 questions cost about \\$0.20 each as of summer, 2023.  \n",
       "\n",
       "\n",
       "#### Attribution Markdown\n",
       "\n",
       "Currently, the attribution is done manually, using the published meeting agenda items for the top headers and the speaker for the sub-headers, making a collapsible presentation possible.  Analytics on speakers and independent topics of a meeting become possible by using attribution, so the effort is worthwhile.  Presenting a 25,000 wall of words has value to feed other processes.  Expanding each agenda item (topic) and arranging the speakers in order with the portion of the transcript attributed to each is eaier to consume for individuals.  As well, the raw transcript is improved with text associated with speakers.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\"transcripts.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa6053-0fc2-4458-9b50-4e0dd959bc8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0009fa-c97b-4037-90fd-fa8694bc6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from sqlalchemy import create_engine\n",
    "import openai\n",
    "\n",
    "##set-up\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(usecwd=True),override=True) # read local .env file\n",
    "\n",
    "openai.api_key  = environ.get('OPENAI_API_KEY','')\n",
    "\n",
    "username     =  environ.get(\"POSTGRES_USERNAME\", \"postgres\")\n",
    "password     =  environ.get(\"POSTGRES_PASSWORD\", \"postgres\")\n",
    "ipaddress    =  environ.get(\"POSTGRES_IPADDRESS\", \"localhost\")\n",
    "port         =  environ.get(\"POSTGRES_PORT\", \"5432\")\n",
    "dbname       =  environ.get(\"POSTGRES_DBNAME\", \"ArlingtonMA\")\n",
    "\n",
    "cnx= create_engine(f'postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5894481-9191-4182-b44c-b529fd30ed3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extract\n",
    "\n",
    "    1. Get youtube video links from hosting agent\n",
    "    2. Use Whisper to create video transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8fdfe-1437-4780-aa72-c776361ac221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For ArlingtonMA the local public cable company is ACMI\n",
    "\n",
    "def extract_acmi_video_urls():\n",
    "    \n",
    "    from pandas import DataFrame, to_datetime\n",
    "    import requests\n",
    "    import re\n",
    "    \n",
    "    stub = \"https://acmi.tv/programs/government/\"\n",
    "    \n",
    "    urls = {\n",
    "        \"school-committee\":\"School Committee Meeting - \",\n",
    "        \"select-board-meetings\":\"Select Board Meeting - \",\n",
    "        \"redevelopment-board-meetings\":\"Redevelopment Board Meeting - \",\n",
    "        \"zoning-board-of-appeals\":\"Zoning Board of Appeals Meeting - \",\n",
    "        \"finance-committee\":\"Finance Committee Meeting - \",\n",
    "        \"town-meeting\":\"Annual Town Meeting - \",\n",
    "    }\n",
    "    \n",
    "    rows = []\n",
    "    for url in urls.keys():\n",
    "        \n",
    "        response = requests.get(stub+url)\n",
    "    \n",
    "        if response.status_code == 200:\n",
    "            # Extract hyperlinks and labels using regular expressions\n",
    "            links_and_labels = re.findall(r'href=[\"\\'](https?://[^\"\\']+)[\"\\'][^<]*<span>(.*?)<\\/span>', response.text)\n",
    "    \n",
    "            for link, label in links_and_labels:\n",
    "                date = to_datetime(label.replace(urls[url],'').replace(\"Special & \",\"\")).date().strftime('%Y-%m-%d')\n",
    "                rows.append({\n",
    "                    \"dor\" : 10,\n",
    "                    \"authority\": url, \n",
    "                    \"date\": date,\n",
    "                    \"video\": link.replace(\"https://www.youtube.com/watch?v=\",\"\"),\n",
    "                })\n",
    "        else:\n",
    "            print(\"Failed to fetch the webpage.\")\n",
    "\n",
    "    df  =  DataFrame(rows)[['dor','authority','date','video']]\n",
    "\n",
    "    ## substitute integer keys for string value\n",
    "    x   =  list(df.authority.sort_values().unique())\n",
    "    x   =  dict(zip(x,range(len(x))))\n",
    "    df  =  df.replace(x).sort_values(['authority','date'])\n",
    "\n",
    "    ## update to common.int_value_pairs; s/b only done once\n",
    "    ivp = DataFrame([x]).T.reset_index().rename(columns={\"index\":\"value\",0:\"key\"})\n",
    "    ivp['item']='authority'\n",
    "\n",
    "    return df, ivp\n",
    "\n",
    "df, int_value_pairs = extract_acmi_video_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4627b8-d421-4a97-b530-aad6a6c8b710",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Read video links from db\n",
    "from pandas import read_sql_query, to_datetime\n",
    "\n",
    "query = \"\"\"\n",
    "            select * from governance.meetings m \n",
    "            left join common.int_value_pairs ivp \n",
    "                on ivp.item='authority' and ivp.key=m.authority \n",
    "            where transcript is null;\n",
    "        \"\"\"\n",
    "meetings = read_sql_query(query,cnx)\n",
    "\n",
    "data_dir   =   \"./meetings/\"\n",
    "authority  =   \"zoning-board-of-appeals\"\n",
    "date       =   \"2023-08-01\"\n",
    "\n",
    "mask       =  (\n",
    "    meetings.value == authority\n",
    ") & ( \n",
    "    meetings.date == to_datetime(date).date()\n",
    ")\n",
    "\n",
    "url_video       =  \"https://www.youtube.com/watch?v=\" + meetings[mask]['video'].iloc[0]\n",
    "save_directory  =  data_dir + authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d11afd-1230-48e2-bc67-946374bb14f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Whisper transcript\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url_video],save_directory),\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print('elapsed',time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da587689-1905-4ab0-acd8-97b723822005",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "    1. Save raw transcript and split documents\n",
    "    2. langchain's RecursiveCharacterTextSplitter on raw transcript\n",
    "    3. create embeddings, store in Chroma vectorstore\n",
    "    4. connect to LLM (gpt-4), create Q&A prompt template\n",
    "    5. RetrievalQA, ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c4d63-e420-4201-bdd6-689b0af7ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from json import dump, loads\n",
    "\n",
    "working_dir = save_directory+'/'+date\n",
    "makedirs(working_dir,exist_ok=True)\n",
    "\n",
    "txt = ' '.join([d.page_content for d in docs])\n",
    "\n",
    "with open(working_dir+'/transcript.txt', 'w') as f:\n",
    "    f.write(txt)\n",
    "\n",
    "makedirs(working_dir+'/docs',exist_ok=True)\n",
    "\n",
    "idx = 0\n",
    "for d in docs:\n",
    "    with open(working_dir+f'/docs/d_{idx}.json', \"w\") as json_file:\n",
    "        dump(loads(d.json()), json_file)\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9603e3e-7a1d-4ab2-ae1f-472bb419dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=7000,\n",
    "    chunk_overlap=1000,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "loader = TextLoader(working_dir+'/transcript.txt')\n",
    "documents = loader.load_and_split(r_splitter)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1907dd-9cb1-4d68-8100-bfc23e3a6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = f'chroma/transcripts/{authority}'\n",
    "\n",
    "!rm -rf ./f'chroma/transcripts/{authority}'  # remove old database files if any\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bfea9-06d4-44ee-a75a-ceff090b27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten sentences maximum. Keep the answer as concise as possible. \n",
    "Try to include the name of everyone who spoke.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913675f8-2965-458c-82a4-d3790e574267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever   =  vectordb.as_retriever(),\n",
    "    chain_type  =  \"stuff\",\n",
    "    chain_type_kwargs = { \"prompt\" : QA_CHAIN_PROMPT }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc9020-4681-406c-9c70-0bac5036d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "question  =  f\"Please, summarize the {authority} meeting.\"\n",
    "result    =  qa_chain({\"query\": question})\n",
    "display(Markdown(result['result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc6ba2-53e7-4fd0-99a6-0e8dab7d2f1c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
